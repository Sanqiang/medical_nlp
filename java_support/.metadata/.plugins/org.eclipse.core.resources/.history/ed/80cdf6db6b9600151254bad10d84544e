package edu.pitt.medical_nlp.text_ex;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.StringReader;
import java.util.ArrayList;
import java.util.List;

import edu.pitt.medical_nlp.graph.Graph;
import edu.pitt.medical_nlp.utility.Config;
import edu.pitt.medical_nlp.utility.Module;
import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.ling.TaggedWord;
import edu.stanford.nlp.parser.nndep.DependencyParser;
import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;
import edu.stanford.nlp.trees.GrammaticalStructure;
import edu.stanford.nlp.trees.TypedDependency;
import edu.stanford.nlp.trees.GrammaticalStructure.Extras;

public class ProcessEx {

	public ProcessEx() {
	}

	void readText() {
		try {
			BufferedReader reader = new BufferedReader(new FileReader(new File(Config.PATH_DATA)));
			String line = null;
			while (null != (line = reader.readLine())) {
				String[] items = line.split("\t");
				String text = items[2];
				// process conj
			}
			reader.close();
		} catch (FileNotFoundException e) {
			e.printStackTrace();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	void processConj(String text) {
		Graph graph = new Graph();
		
		MaxentTagger tagger = Module.getInst().getTagger();
		DependencyParser parser = Module.getInst().getDependencyParser();
		DocumentPreprocessor tokenizer = new DocumentPreprocessor(new StringReader(text));
		for (List<HasWord> sentence : tokenizer) {
		      List<TaggedWord> tagged = tagger.tagSentence(sentence);
		      GrammaticalStructure gs = parser.predict(tagged);
		      for (TypedDependency typed_dependence : gs.typedDependenciesCCprocessed(Extras.MAXIMAL)) {
					int idx_gov = typed_dependence.gov().index() - 1;
					int idx_dep = typed_dependence.dep().index() - 1;
					
		      }
		}
	}
}
