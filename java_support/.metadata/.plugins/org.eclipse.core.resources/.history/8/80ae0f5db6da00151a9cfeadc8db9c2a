package medical_classify;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.StringReader;
import java.util.ArrayList;
import java.util.List;

import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.ling.TaggedWord;
import edu.stanford.nlp.ling.Word;
import edu.stanford.nlp.parser.nndep.DependencyParser;
import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;
import edu.stanford.nlp.trees.GrammaticalStructure;
import edu.stanford.nlp.trees.TypedDependency;
import medical_classify.structure.Edge;
import medical_classify.structure.Graph;
import edu.stanford.nlp.trees.GrammaticalStructure.Extras;

public class Process {

	protected ArrayList<String> _docs = null;

	public Process() {
		
		

		try {
			BufferedWriter writer = new BufferedWriter(new FileWriter(new File("data/data_neg2.txt")));
			BufferedReader reader = new BufferedReader(new FileReader(new File(Config.PATH_DATA)));
			String line = null;
			while (null != (line = reader.readLine())) {
				String[] items = line.split("\t");
				String doc = processSentence(items[2]);
				

			}
			reader.close();
		} catch (FileNotFoundException e) {
			e.printStackTrace();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	MaxentTagger tagger = Module.getInst().getTagger();
	DependencyParser parser = Module.getInst().getDependencyParser();


	public String processSentence(String doc) {
		StringBuilder sb = new StringBuilder();
		DocumentPreprocessor tokenizer = new DocumentPreprocessor(new StringReader(doc));
		for (List<HasWord> sentence : tokenizer) {
			List<TaggedWord> tagged = tagger.tagSentence(sentence);
			Graph graph = new Graph();
			GrammaticalStructure gs = parser.predict(tagged);
			for (TypedDependency typed_dependence : gs.typedDependenciesCCprocessed(Extras.MAXIMAL)) {
				int idx_gov = typed_dependence.gov().index() - 1;
				int idx_dep = typed_dependence.dep().index() - 1;
				if (idx_dep >= 0 && idx_gov >= 0 && idx_dep < sentence.size() && idx_gov < sentence.size()) {
					HasWord node_gov = sentence.get(idx_gov);
					HasWord node_dep = sentence.get(idx_dep);
					String lemma_gov = typed_dependence.gov().word(), lemma_dep = typed_dependence.dep().word();
					if (!node_gov.word().equals(lemma_gov) || !node_dep.word().equals(lemma_dep)) {
						System.err.println(sentence);
					}
					String dependency_type = typed_dependence.reln().getShortName();

					graph.addEdge(idx_gov, lemma_gov, idx_dep, lemma_dep, dependency_type);
				}
			}

			// remove neg
			ArrayList<Integer> remove_idxs = new ArrayList<>();
			for (Edge edge : graph.edges) {
				if (edge.type.equals("neg")) {
					remove_idxs.add(edge.vertex1.idx);
					remove_idxs.add(edge.vertex2.idx);

					// delete associate
					// for (Edge edge2 : edge.vertex1.edges) {
					// if (edge2.type.equals("nmod") ||
					// edge2.type.equals("conj")) {
					// remove_idxs.add(edge2.vertex1.idx);
					// remove_idxs.add(edge2.vertex2.idx);
					// }
					// }
					//
					// for (Edge edge2 : edge.vertex2.edges) {
					// if (edge2.type.equals("nmod") ||
					// edge2.type.equals("conj")) {
					// remove_idxs.add(edge2.vertex1.idx);
					// remove_idxs.add(edge2.vertex2.idx);
					// }
					// }
					// }
				}

				for (int remove_idx : remove_idxs) {
					sentence.set(remove_idx, new Word(" "));
				}
			}
			for (HasWord word : sentence) {
				//System.out.print(word.word() + " ");
				sb.append(word.word()).append(" ");
			}
		}
		sb.append("\n");
		return sb.toString();
	}

	public static void main(String[] args) {
		Process process = new Process();
		process.processDocs();
		//process.processSentence("no AAA and BBB");
	}

}
